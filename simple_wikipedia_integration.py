#!/usr/bin/env python3
"""
Simple Wikipedia to Metadata Integration
Add Wikipedia content to documents_metadata.json
"""

import os
import json
from datetime import datetime

def main():
    print("ğŸš€ ARQA Wikipedia Integration")
    print("=" * 60)
    
    # Load existing metadata
    print("ğŸ“ Loading existing metadata...")
    try:
        with open('documents_metadata.json', 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
        existing_count = len(existing_data.get('documents', []))
        print(f"   âœ… Loaded {existing_count:,} existing documents")
    except Exception as e:
        print(f"   âŒ Error loading metadata: {e}")
        return
    
    # Load Wikipedia chunks
    print("ğŸ“ Loading Wikipedia chunks...")
    all_chunks = []
    data_dir = "wikipedia_test"
    
    if not os.path.exists(data_dir):
        print(f"âŒ Directory not found: {data_dir}")
        return
    
    batch_files = [f for f in os.listdir(data_dir) if f.startswith('wikipedia_batch_') and f.endswith('.json')]
    batch_files.sort()
    
    for batch_file in batch_files:
        batch_path = os.path.join(data_dir, batch_file)
        try:
            with open(batch_path, 'r', encoding='utf-8') as f:
                batch_data = json.load(f)
                all_chunks.extend(batch_data)
                print(f"   âœ… Loaded {len(batch_data)} chunks from {batch_file}")
        except Exception as e:
            print(f"   âŒ Error loading {batch_file}: {e}")
    
    print(f"ğŸ“„ Total Wikipedia chunks loaded: {len(all_chunks)}")
    
    # Convert to metadata format
    print("ğŸ”„ Converting Wikipedia chunks to metadata format...")
    new_documents = []
    
    for i, chunk in enumerate(all_chunks):
        doc_id = existing_count + i
        metadata = chunk.get('metadata', {})
        title = metadata.get('title', 'Unknown')
        
        document = {
            "id": f"doc_{doc_id}",
            "content": chunk.get('content', ''),
            "meta": {
                "source": "wikipedia",
                "title": title,
                "wikipedia_metadata": metadata,
                "added_date": datetime.now().isoformat(),
                "chunk_id": metadata.get('chunk_id', 0),
                "total_chunks": metadata.get('total_chunks', 1)
            },
            "chunk_id": metadata.get('chunk_id', 0)
        }
        
        new_documents.append(document)
        
        if (i + 1) % 1000 == 0:
            print(f"   ğŸ“ˆ Converted {i + 1:,} chunks...")
    
    print(f"âœ… Conversion complete: {len(new_documents)} documents ready")
    
    # Show integration plan
    total_after = existing_count + len(new_documents)
    print(f"\nğŸ¯ Integration Plan:")
    print(f"   ğŸ“„ Existing documents: {existing_count:,}")
    print(f"   ğŸ“„ New Wikipedia documents: {len(new_documents):,}")
    print(f"   ğŸ“„ Total after merge: {total_after:,}")
    
    # Confirm integration
    confirm = input(f"\nâœ… Proceed with integration? (y/n): ").lower().strip()
    if confirm != 'y':
        print("âŒ Integration cancelled")
        return
    
    # Create backup
    backup_file = f"documents_metadata.json.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    try:
        with open('documents_metadata.json', 'r', encoding='utf-8') as src:
            with open(backup_file, 'w', encoding='utf-8') as dst:
                dst.write(src.read())
        print(f"ğŸ’¾ Backup created: {backup_file}")
    except Exception as e:
        print(f"âš ï¸ Backup failed: {e}")
    
    # Merge and save
    print("ğŸ”— Merging and saving...")
    merged_data = existing_data.copy()
    merged_data['documents'].extend(new_documents)
    
    try:
        with open('documents_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Successfully saved {len(merged_data['documents']):,} documents")
        
        # Validation
        print("ğŸ” Validating integration...")
        doc_ids = [doc['id'] for doc in merged_data['documents']]
        unique_ids = set(doc_ids)
        wikipedia_docs = [doc for doc in merged_data['documents'] if doc.get('meta', {}).get('source') == 'wikipedia']
        
        print(f"ğŸ“Š Validation Results:")
        print(f"   ğŸ“„ Total documents: {len(merged_data['documents']):,}")
        print(f"   ğŸ†” Unique IDs: {len(unique_ids):,}")
        print(f"   ğŸ“š Wikipedia documents: {len(wikipedia_docs):,}")
        print(f"   âœ… No ID duplicates: {len(unique_ids) == len(merged_data['documents'])}")
        
        if len(unique_ids) == len(merged_data['documents']):
            print(f"\nğŸ‰ Wikipedia Integration Complete!")
            print(f"âœ… {len(new_documents):,} Wikipedia documents added successfully")
            print(f"ğŸ“„ Total dataset now contains {len(merged_data['documents']):,} documents")
            print(f"ğŸš€ Ready for enhanced Arabic question answering!")
        else:
            print(f"\nâŒ Integration completed but validation failed")
            
    except Exception as e:
        print(f"âŒ Error saving metadata: {e}")

if __name__ == "__main__":
    main()
