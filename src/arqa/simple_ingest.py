"""
Simplified HTML Ingestion Module for ARQA
Enhanced with PyArabic for better Arabic text normalization
"""

from typing import List, Dict, Any, Optional
import os
import re
import json
from pathlib import Path
import unicodedata
from bs4 import BeautifulSoup
import pyarabic.araby as araby


class SimpleDocumentIngestor:
    """ğŸš€ Simplified Arabic HTML document preprocessing and ingestion with PyArabic."""
    
    def __init__(self, 
                 output_dir: str = "processed_documents",
                 chunk_size: int = 200):
        """
        ğŸ”§ Initialize the simplified document ingestor with PyArabic.
        
        Args:
            output_dir: Directory to store processed documents
            chunk_size: Target tokens per chunk
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.chunk_size = chunk_size
        
        # ğŸ“‹ Using PyArabic for enhanced Arabic text normalization
        print("ğŸ”§ Using PyArabic for enhanced Arabic text normalization")
    
    def extract_html_content(self, html_content: str) -> Dict[str, Any]:
        """
        ğŸ•¸ï¸ Extract clean text and metadata from HTML.
        
        Args:
            html_content: Raw HTML string
            
        Returns:
            Dict with cleaned text and metadata
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ğŸ—‘ï¸ Remove script and style elements
        for script in soup(["script", "style", "nav", "header", "footer"]):
            script.decompose()
        
        # ğŸ“ Extract title
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "No Title"
          # ğŸ“„ Extract all text content from the entire HTML document
        # Get clean text from the entire document after removing unwanted elements
        text = soup.get_text(separator=' ', strip=True)
          # ğŸ·ï¸ Extract metadata
        metadata = {
            'title': title_text,
            'html_length': len(html_content),
            'text_length': len(text)
        }        # ğŸ“Š Try to extract meta tags
        try:
            # Simple metadata extraction - skip if errors occur
            pass
        except Exception:
            # If meta extraction fails, continue without metadata
            pass
        
        return {
            'text': text,
            'metadata': metadata
        }
    
    def normalize_arabic_text(self, text: str) -> str:
        """
        ğŸ”¤ Normalize Arabic text using PyArabic library.
        
        Args:
            text: Raw Arabic text
            
        Returns:
            Normalized Arabic text using PyArabic
        """
        if not text:
            return ""
        
        # ğŸ“ Apply Unicode normalization first
        normalized = unicodedata.normalize('NFKC', text)
        
        # ğŸ”„ Use PyArabic for comprehensive Arabic text normalization
        # Remove diacritics (tashkeel)
        normalized = araby.strip_tashkeel(normalized)
        
        # Remove tatweel (kashida) - elongation character
        normalized = araby.strip_tatweel(normalized)
        
        # Normalize Arabic letters (hamza forms, etc.)
        normalized = araby.normalize_hamza(normalized)
        normalized = araby.normalize_alef(normalized)
        normalized = araby.normalize_teh(normalized)
        
        # Additional Arabic-specific cleaning
        # Convert different forms of yaa
        normalized = normalized.replace('Ù‰', 'ÙŠ')  # Alif maksura to yaa
        
        
        # ğŸ§½ Clean extra whitespace
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized
    
    def simple_tokenize(self, text: str) -> List[str]:
        """
        ğŸ”¤ Arabic-aware tokenization using PyArabic.
        
        Args:
            text: Input text
              Returns:
            List of tokens
        """
        # Use PyArabic's tokenizer for better Arabic handling
        tokens = araby.tokenize(text)
        
        # Filter out empty tokens and very short tokens
        tokens = [token.strip() for token in tokens if token.strip() and len(token.strip()) > 1]
        
        return tokens

    def chunk_text_by_tokens(self, text: str, chunk_size: Optional[int] = None, overlap: int = 50) -> List[str]:
        """
        âœ‚ï¸ Split text into overlapping token-based chunks.
        
        Args:
            text: Input text
            chunk_size: Target tokens per chunk
            overlap: Overlap tokens between chunks
            
        Returns:
            List of text chunks
        """
        if chunk_size is None:
            chunk_size = self.chunk_size
        
        tokens = self.simple_tokenize(text)
        
        if len(tokens) <= chunk_size:
            return [text]
        
        chunks = []
        start_idx = 0
        
        while start_idx < len(tokens):
            # ğŸ“ Calculate chunk boundaries
            end_idx = min(start_idx + chunk_size, len(tokens))
            chunk_tokens = tokens[start_idx:end_idx]
            
            # ğŸ”— Join tokens back to text
            chunk_text = ' '.join(chunk_tokens)
            chunks.append(chunk_text)
            
            # ğŸ“ Move start position (with overlap)
            start_idx += chunk_size - overlap
            
            # ğŸ›‘ Break if we've covered all tokens
            if end_idx >= len(tokens):
                break
        
        return chunks
    
    def process_html_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        ğŸ“„ Process a single HTML file.
        
        Args:
            file_path: Path to HTML file
            
        Returns:
            List of processed document chunks
        """
        try:            # ğŸ“– Read HTML file
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # ğŸ•¸ï¸ Extract content from HTML
            extracted = self.extract_html_content(html_content)
            
            # ğŸ”¤ Keep original text for non-normalized answers
            original_text = extracted['text']
            
            # âœ‚ï¸ Create chunks from original text
            chunks = self.chunk_text_by_tokens(original_text)
            
            # ğŸ“¦ Create document objects
            documents = []
            for i, chunk in enumerate(chunks):
                doc = {
                    'content': chunk,
                    'metadata': {
                        **extracted['metadata'],
                        'source_file': file_path,
                        'filename': os.path.basename(file_path),
                        'chunk_id': i,
                        'total_chunks': len(chunks),
                        'chunk_length': len(chunk.split())
                    }
                }
                documents.append(doc)
            
            print(f"âœ… Processed {file_path}: {len(documents)} chunks")
            return documents
            
        except Exception as e:
            print(f"âŒ Error processing {file_path}: {e}")
            return []
    
    def ingest_from_directory(self, directory_path: str, file_pattern: str = "*.html") -> List[Dict[str, Any]]:
        """
        ğŸ“ Process all HTML files from a directory.
        
        Args:
            directory_path: Path to directory containing HTML files
            file_pattern: Glob pattern for file matching
            
        Returns:
            List of all processed documents
        """
        directory = Path(directory_path)
        html_files = list(directory.glob(file_pattern))
        
        if not html_files:
            print(f"âŒ No HTML files found in {directory_path}")
            return []
        
        print(f"ğŸ“ Found {len(html_files)} HTML files in {directory_path}")
        
        all_documents = []
        successful_files = 0
        
        for i, file_path in enumerate(html_files, 1):
            print(f"ğŸ“„ Processing {i}/{len(html_files)}: {file_path.name}")
            
            documents = self.process_html_file(str(file_path))
            if documents:
                all_documents.extend(documents)
                successful_files += 1
        
        # ğŸ’¾ Save processed documents
        self.save_documents(all_documents)
        
        print(f"ğŸ‰ Processing complete!")
        print(f"ğŸ“Š Successfully processed: {successful_files}/{len(html_files)} files")
        print(f"ğŸ“„ Total chunks created: {len(all_documents)}")
        
        return all_documents
    
    def save_documents(self, documents: List[Dict[str, Any]]) -> None:
        """
        ğŸ’¾ Save processed documents to JSON file.
        
        Args:
            documents: List of processed documents
        """
        output_file = self.output_dir / "processed_documents.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Saved {len(documents)} documents to {output_file}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """ğŸ“ˆ Get processing statistics."""
        output_file = self.output_dir / "processed_documents.json"
        
        if output_file.exists():
            with open(output_file, 'r', encoding='utf-8') as f:
                documents = json.load(f)
            
            return {
                'total_documents': len(documents),
                'output_directory': str(self.output_dir),
                'output_file': str(output_file)
            }
        else:
            return {
                'total_documents': 0,
                'output_directory': str(self.output_dir),
                'output_file': str(output_file)
            }
    
    def process_html_content(self, html_content: str, source_url: str = "uploaded_file") -> List[Dict[str, Any]]:
        """
        ğŸ“„ Process HTML content directly (for API uploads).
        
        Args:
            html_content: Raw HTML content string
            source_url: Source identifier for the content
              Returns:
            List of processed document chunks
        """
        try:
            # ğŸ•¸ï¸ Extract content from HTML
            extracted = self.extract_html_content(html_content)
            
            # ğŸ”¤ Keep original text for non-normalized answers
            original_text = extracted['text']
            
            # âœ‚ï¸ Create chunks from original text
            chunks = self.chunk_text_by_tokens(original_text)
            
            # ğŸ“¦ Create document objects
            documents = []
            for i, chunk in enumerate(chunks):
                doc = {
                    'content': chunk,
                    'metadata': {
                        **extracted['metadata'],
                        'source_file': source_url,
                        'filename': source_url,                        'chunk_id': i,
                        'total_chunks': len(chunks),
                        'chunk_length': len(chunk.split())
                    }
                }
                documents.append(doc)
            
            print(f"âœ… Processed HTML content: {len(documents)} chunks")
            return documents
            
        except Exception as e:
            print(f"âŒ Error processing HTML content: {e}")
            return []
    
    def extract_xml_content(self, xml_content: str) -> Dict[str, Any]:
        """
        ğŸ”§ Extract clean text and metadata from XML.
        
        Args:
            xml_content: Raw XML string
            
        Returns:
            Dict with cleaned text and metadata
        """
        try:
            # Try XML parser first, fallback to html.parser if XML parser fails
            try:
                soup = BeautifulSoup(xml_content, 'xml')
            except:
                soup = BeautifulSoup(xml_content, 'html.parser')
        except Exception as e:
            print(f"âš ï¸ XML parsing warning: {e}")
            # Fallback to simple text extraction
            import re
            text = re.sub(r'<[^>]+>', ' ', xml_content)
            text = re.sub(r'\s+', ' ', text).strip()
            return {
                'text': text,
                'metadata': {
                    'title': 'XML Document',
                    'xml_length': len(xml_content),
                    'text_length': len(text),
                    'file_type': 'xml'
                }
            }
        
        # ğŸ—‘ï¸ Remove script and style elements if present
        for script in soup(["script", "style"]):
            script.decompose()
        
        # ğŸ“ Try to extract title from various XML elements
        title = None
        title_candidates = ['title', 'name', 'header', 'subject', 'h1', 'h2']
        for candidate in title_candidates:
            title = soup.find(candidate)
            if title:
                break
        
        title_text = title.get_text().strip() if title else "XML Document"
        
        # ğŸ“„ Extract all text content from XML
        text = soup.get_text(separator=' ', strip=True)
          # ğŸ·ï¸ Extract metadata
        metadata = {
            'title': title_text,
            'xml_length': len(xml_content),
            'text_length': len(text),
            'file_type': 'xml'
        }
        
        # ğŸ“Š Try to extract basic metadata
        # Skip complex attribute extraction to avoid compatibility issues
        
        return {
            'text': text,
            'metadata': metadata
        }
    
    def process_xml_content(self, xml_content: str, source_url: str = "uploaded_file") -> List[Dict[str, Any]]:
        """
        ğŸ“„ Process XML content directly (for API uploads).
        
        Args:
            xml_content: Raw XML content string
            source_url: Source identifier for the content
            
        Returns:
            List of processed document chunks
        """
        try:
            # ğŸ”§ Extract content from XML
            extracted = self.extract_xml_content(xml_content)
            
            # ğŸ”¤ Keep original text for non-normalized answers
            original_text = extracted['text']
            
            # âœ‚ï¸ Create chunks from original text
            chunks = self.chunk_text_by_tokens(original_text)
            
            # ğŸ“¦ Create document objects
            documents = []
            for i, chunk in enumerate(chunks):
                doc = {
                    'content': chunk,
                    'metadata': {
                        **extracted['metadata'],
                        'source_file': source_url,
                        'filename': source_url,
                        'chunk_id': i,
                        'total_chunks': len(chunks),
                        'chunk_length': len(chunk.split())
                    }
                }
                documents.append(doc)
            
            print(f"âœ… Processed XML content: {len(documents)} chunks")
            return documents
            
        except Exception as e:
            print(f"âŒ Error processing XML content: {e}")
            return []
    
    def process_xml_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        ğŸ“„ Process a single XML file.
        
        Args:
            file_path: Path to XML file
            
        Returns:
            List of processed document chunks
        """
        try:
            # ğŸ“– Read XML file
            with open(file_path, 'r', encoding='utf-8') as f:
                xml_content = f.read()
            
            # ğŸ”§ Extract content from XML
            extracted = self.extract_xml_content(xml_content)
            
            # ğŸ”¤ Keep original text for non-normalized answers
            original_text = extracted['text']
            
            # âœ‚ï¸ Create chunks from original text
            chunks = self.chunk_text_by_tokens(original_text)
            
            # ğŸ“¦ Create document objects
            documents = []
            for i, chunk in enumerate(chunks):
                doc = {
                    'content': chunk,
                    'metadata': {
                        **extracted['metadata'],
                        'source_file': file_path,
                        'filename': os.path.basename(file_path),
                        'chunk_id': i,
                        'total_chunks': len(chunks),
                        'chunk_length': len(chunk.split())
                    }
                }
                documents.append(doc)
            
            print(f"âœ… Processed {file_path}: {len(documents)} chunks")
            return documents
            
        except Exception as e:
            print(f"âŒ Error processing {file_path}: {e}")
            return []

# Create an alias for compatibility
DocumentIngestor = SimpleDocumentIngestor


# ğŸ¯ Example usage script
if __name__ == "__main__":
    # ğŸš€ Initialize simple ingestor
    ingestor = SimpleDocumentIngestor()
    
    # ğŸ“ Process HTML files from directory
    html_directory = input("ğŸ“ Enter path to HTML files directory: ").strip()
    
    if os.path.exists(html_directory):
        print("ğŸ™ï¸ Starting simplified Arabic HTML ingestion...")
        documents = ingestor.ingest_from_directory(html_directory)
        
        # ğŸ“Š Show statistics
        stats = ingestor.get_statistics()
        print(f"ğŸ“ˆ Final stats: {stats}")
    else:
        print("âŒ Directory not found!")
