#!/usr/bin/env python3
"""
Test PyArabic Integration in ARQA
Validates the enhanced Arabic normalization using PyArabic
"""

import sys
import os

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.arqa.simple_ingest import SimpleDocumentIngestor

def test_pyarabic_normalization():
    print("ğŸ§ª Testing PyArabic Integration in ARQA")
    print("=" * 50)
    
    # Initialize the ingestor
    print("1ï¸âƒ£ Initializing Document Ingestor with PyArabic...")
    ingestor = SimpleDocumentIngestor()
    
    # Test Arabic text samples with various issues
    test_texts = [
        "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙƒÙ… ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯",  # Mixed diacritics
        "Ø§Ù„Ø°ÙÙ‘ÙƒØ§Ø¡Ù Ø§Ù„Ø§ØµÙ’Ø·ÙÙ†Ø§Ø¹ÙÙŠÙÙ‘ Ù…ÙÙ‡ÙÙ…ÙŒÙ‘ Ø¬ÙØ¯Ù‹Ù‘Ø§",  # Heavy diacritics
        "ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© ØµØ­ÙŠØ­Ø©",  # Mixed forms
        "Ù‡Ø°Ø§ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©",  # Clean text
        "Ø§Ù„ØªØ¹Ù„ÙÙ‘Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",  # Mixed hamza forms
        "Ù†ØµÙŒÙ‘ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"  # Various diacritics
    ]
    
    print("2ï¸âƒ£ Testing Arabic Text Normalization...")
    print()
    
    for i, text in enumerate(test_texts, 1):
        print(f"Test {i}:")
        print(f"   Original: {text}")
        
        # Normalize using PyArabic
        normalized = ingestor.normalize_arabic_text(text)
        print(f"   Normalized: {normalized}")
        
        # Tokenize using PyArabic
        tokens = ingestor.simple_tokenize(text)
        print(f"   Tokens ({len(tokens)}): {tokens[:5]}...")  # Show first 5 tokens
        print()
    
    print("3ï¸âƒ£ Testing HTML Processing with PyArabic...")
    
    # Test with a sample HTML content
    sample_html = """
    <!DOCTYPE html>
    <html lang="ar">
    <head>
        <title>Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ</title>
        <meta name="description" content="Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©">
    </head>
    <body>
        <h1>Ø§Ù„Ø°ÙÙ‘ÙƒØ§Ø¡Ù Ø§Ù„Ø§ØµÙ’Ø·ÙÙ†Ø§Ø¹ÙÙŠÙÙ‘</h1>
        <p>Ù‡Ø°Ø§ Ù†ØµÙŒÙ‘ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© PyArabic.</p>
        <p>Ø§Ù„Ù†ØµÙÙ‘ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ£Ø´ÙƒØ§Ù„ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.</p>
    </body>
    </html>
    """
    
    # Process the HTML content
    documents = ingestor.process_html_content(sample_html, "test_pyarabic")
    
    if documents:
        print(f"   âœ… Successfully processed HTML content")
        print(f"   ğŸ“„ Created {len(documents)} document chunks")
        
        for i, doc in enumerate(documents):
            content = doc['content'][:100]  # First 100 chars
            print(f"   Chunk {i+1}: {content}...")
    else:
        print("   âŒ Failed to process HTML content")
    
    print("\n4ï¸âƒ£ Performance Comparison...")
    
    # Test performance with a longer text
    long_text = " ".join(test_texts * 10)  # Repeat texts 10 times
    
    import time
    
    # Test normalization speed
    start_time = time.time()
    normalized_long = ingestor.normalize_arabic_text(long_text)
    normalization_time = time.time() - start_time
    
    # Test tokenization speed
    start_time = time.time()
    tokens_long = ingestor.simple_tokenize(long_text)
    tokenization_time = time.time() - start_time
    
    print(f"   Text length: {len(long_text)} characters")
    print(f"   Normalization time: {normalization_time:.4f}s")
    print(f"   Tokenization time: {tokenization_time:.4f}s")
    print(f"   Total tokens: {len(tokens_long)}")
    
    print("\nğŸ‰ PyArabic Integration Test Complete!")
    print("âœ… Enhanced Arabic normalization is working!")

if __name__ == "__main__":
    test_pyarabic_normalization()
