# ğŸš€ ARQA Enhanced HTML Ingestion Setup Guide

This guide explains how to set up and use the enhanced Arabic Question Answering (ARQA) system with HTML ingestion capabilities.

## ğŸ“‹ Prerequisites

Make sure you have Python 3.8+ installed on your system.

## ğŸ”§ Installation

1. **Clone or navigate to the ARQA directory:**
   ```bash
   cd c:\Users\a-ahm\Desktop\arqa
   ```

2. **Install required dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install additional Arabic language dependencies:**
   ```bash
   # For CAMEL-tools Arabic processing
   pip install camel-tools[cli]
   
   # For Farasa Arabic segmentation
   pip install farasa==0.0.9
   ```

## ğŸ¯ Enhanced HTML Ingestion Features

The enhanced `ingest.py` module now includes:

### âœ¨ Key Features:
- ğŸ•¸ï¸ **HTML Content Extraction**: Smart extraction from HTML files with BeautifulSoup
- ğŸ”¤ **Arabic Text Normalization**: Comprehensive normalization of Hamza, Ta-marbuta, etc.
- âœ‚ï¸ **Smart Chunking**: 200-token chunks with 25% overlap for optimal retrieval
- ğŸ’¾ **Dual Storage**: FAISS for semantic search + SQLite for metadata queries
- ğŸ“Š **Rich Metadata**: Automatic extraction of titles, descriptions, keywords
- ğŸ™ï¸ **Emoji Flow**: Clear emoji-commented methods for easy code reading

### ğŸ—ï¸ Architecture:
```
DocumentIngestor
â”œâ”€â”€ ğŸ•¸ï¸ extract_html_content()     # Clean HTML â†’ text + metadata
â”œâ”€â”€ ğŸ”¤ normalize_arabic_text()    # Arabic text normalization
â”œâ”€â”€ âœ‚ï¸ chunk_text_by_tokens()     # Smart 200-token chunking
â”œâ”€â”€ ğŸ“„ ingest_html_file()         # Process single HTML file
â”œâ”€â”€ ğŸ“š ingest_html_articles()     # Batch process multiple files
â”œâ”€â”€ ğŸ“ ingest_from_directory()    # Process directory of HTML files
â””â”€â”€ ğŸ’¾ save_index()               # Persist FAISS index
```

## ğŸ“š Usage Examples

### 1. Basic HTML Directory Processing

```python
from src.arqa.ingest import DocumentIngestor

# Initialize ingestor
ingestor = DocumentIngestor(
    faiss_index_path="./indices/arabic_articles",
    sqlite_path="./data/articles.db"
)

# Process all HTML files in a directory
ingestor.ingest_from_directory("./path/to/html/articles")

# Save the index
ingestor.save_index()

# Get statistics
stats = ingestor.get_statistics()
print(f"Processed {stats['total_documents']} document chunks")
```

### 2. Single File Processing

```python
# Process a single HTML file
ingestor.ingest_html_file(
    "article.html", 
    metadata={"category": "technology", "source": "news_site"}
)
```

### 3. Batch Processing with Custom Metadata

```python
html_files = [
    "tech_article1.html",
    "tech_article2.html", 
    "science_article1.html"
]

ingestor.ingest_html_articles(html_files)
```

## ğŸ§ª Testing the System

Run the included test script to verify everything works:

```bash
python test_html_ingestion.py
```

This will:
- Create sample Arabic HTML articles
- Process them through the ingestion pipeline
- Show statistics and results
- Demonstrate the full workflow

## ğŸ“Š Text Processing Pipeline

### Input HTML:
```html
<article>
    <h1>Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</h1>
    <p>ÙŠÙØ¹ØªØ¨Ø± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† Ø£Ù‡Ù… Ø§Ù„ØªØ·ÙˆØ±Ø§Øª...</p>
</article>
```

### Processing Steps:
1. **ğŸ•¸ï¸ HTML Extraction**: Remove tags, extract clean text
2. **ğŸ”¤ Arabic Normalization**: 
   - `Ø£Ø¥Ø¢` â†’ `Ø§` (Hamza normalization)
   - `Ø©` â†’ `Ù‡` (Ta-marbuta normalization)
   - Remove diacritics and extra whitespace
3. **âœ‚ï¸ Smart Chunking**: Split into 200-token chunks with 50-token overlap
4. **ğŸ’¾ Storage**: Save to FAISS (vectors) + SQLite (metadata)

### Output Structure:
```json
{
    "content": "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù‡Ù… Ø§Ù„ØªØ·ÙˆØ±Ø§Øª...",
    "metadata": {
        "title": "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        "chunk_id": 0,
        "total_chunks": 3,
        "source_file": "article.html",
        "chunk_length": 187
    }
}
```

## ğŸ›ï¸ Configuration Options

### DocumentIngestor Parameters:
- `faiss_index_path`: Path to store FAISS vector index
- `sqlite_path`: Path to SQLite metadata database  
- `embedding_dim`: Vector embedding dimensions (default: 768)

### Chunking Parameters:
- `chunk_size`: Tokens per chunk (default: 200)
- `overlap`: Overlap between chunks (default: 50 = 25%)

## ğŸ“ Output Files

After processing, you'll have:
- `faiss_index/`: FAISS vector index files
- `documents.db`: SQLite database with metadata
- Processing logs showing statistics

## ğŸ” Next Steps

Once ingestion is complete, you can:
1. **Phase 2**: Use `retriever.py` for semantic search
2. **Phase 3**: Use `reader.py` for question answering
3. **Phase 4**: Start the FastAPI service with `api.py`

## ğŸ› ï¸ Troubleshooting

### Common Issues:

1. **Import Errors**: Make sure all dependencies are installed
   ```bash
   pip install -r requirements.txt
   ```

2. **Arabic Text Issues**: Ensure HTML files are UTF-8 encoded

3. **Memory Issues**: For large datasets, process in smaller batches

4. **FAISS Index Errors**: Delete existing index files and recreate

### Debug Mode:
Enable verbose logging by modifying the ingestor:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ Support

For issues or questions:
- Check the documentation in `docs/` directory
- Review the phase-specific guides
- Examine the test script for usage examples

---
ğŸ‰ **Congratulations!** Your enhanced ARQA HTML ingestion system is ready to process Arabic web content with state-of-the-art normalization and chunking!
