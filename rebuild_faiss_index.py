#!/usr/bin/env python3
"""
Rebuild FAISS Index with Wikipedia Content
Update the FAISS index to include all documents from documents_metadata.json
"""

import os
import sys
import json
from typing import List, Dict, Any

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def main():
    print("ğŸ”§ ARQA FAISS Index Rebuild")
    print("=" * 60)
    
    # Import the retriever
    try:
        from arqa.retriever_optimized_fixed import OptimizedArabicRetriever
        print("âœ… Successfully imported OptimizedArabicRetriever")
    except ImportError as e:
        print(f"âŒ Error importing retriever: {e}")
        return
    
    # Load documents from metadata
    metadata_file = "documents_metadata.json"
    print(f"ğŸ“ Loading documents from: {metadata_file}")
    
    try:
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        documents = metadata.get('documents', [])
        model_name = metadata.get('model_name', 'abdoelsayed/AraDPR')
        
        print(f"âœ… Loaded {len(documents):,} documents")
        print(f"ğŸ¤– Model: {model_name}")
        
    except FileNotFoundError:
        print(f"âŒ Metadata file not found: {metadata_file}")
        return
    except Exception as e:
        print(f"âŒ Error loading metadata: {e}")
        return
    
    # Initialize retriever (this will create a fresh index)
    print(f"\nğŸ”§ Initializing retriever...")
    try:
        retriever = OptimizedArabicRetriever(
            model_name=model_name,
            index_path="./faiss_index",
            documents_path="./documents_metadata.json",
            batch_size=64,  # Larger batch for faster processing
            device="auto"
        )
        print(f"âœ… Retriever initialized")
        
    except Exception as e:
        print(f"âŒ Error initializing retriever: {e}")
        return
    
    # Check current index status
    current_docs = len(retriever.documents) if hasattr(retriever, 'documents') else 0
    print(f"ğŸ“Š Current index contains: {current_docs:,} documents")
    print(f"ğŸ“Š Metadata contains: {len(documents):,} documents")
    
    if current_docs == len(documents):
        print("âœ… Index is already up to date!")
        return
    
    # Confirm rebuild
    print(f"\nğŸ¯ Rebuild Plan:")
    print(f"   ğŸ“„ Documents to index: {len(documents):,}")
    print(f"   ğŸ¤– Model: {model_name}")
    print(f"   ğŸ“ Index path: ./faiss_index.faiss")
    
    # Clear existing index if it exists
    index_file = "./faiss_index.faiss"
    if os.path.exists(index_file):
        print(f"ğŸ—‘ï¸ Removing existing index file: {index_file}")
        try:
            os.remove(index_file)
        except Exception as e:
            print(f"âš ï¸ Could not remove existing index: {e}")
    
    # Reset retriever to clear any loaded data
    retriever.index = None
    retriever.documents = []
    retriever.id_to_doc = {}
    retriever.document_hashes = set()
    
    confirm = input(f"\nâœ… Proceed with rebuilding the index? (y/n): ").lower().strip()
    if confirm != 'y':
        print("âŒ Index rebuild cancelled")
        return
    
    # Add all documents
    print(f"\nğŸš€ Starting index rebuild...")
    try:
        result = retriever.add_documents_incremental(
            documents=documents,
            background=False,  # Process immediately
            force_reindex=True  # Force reindexing even if documents exist
        )
        
        print(f"\nğŸ‰ Index Rebuild Complete!")
        print(f"âœ… Processed documents: {result.get('new_documents', 0):,}")
        print(f"â±ï¸ Processing time: {result.get('processing_time', 0):.2f} seconds")
        
        # Verify the rebuilt index
        print(f"\nğŸ” Verifying rebuilt index...")
        final_count = len(retriever.documents)
        index_size = retriever.index.ntotal if retriever.index else 0
        
        print(f"ğŸ“Š Final Statistics:")
        print(f"   ğŸ“„ Total documents: {final_count:,}")
        print(f"   ğŸ“Š Index size: {index_size:,}")
        print(f"   âœ… Match: {final_count == index_size}")
        
        if final_count == len(documents):
            print(f"ğŸ¯ Success! Index contains all {len(documents):,} documents")
        else:
            print(f"âš ï¸ Warning: Expected {len(documents):,} documents, but index has {final_count:,}")
        
        # Test search functionality
        print(f"\nğŸ§ª Testing search functionality...")
        try:
            test_query = "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø§Ø¡ØŸ"
            results = retriever.retrieve(test_query, top_k=3)
            
            print(f"âœ… Search test successful! Found {len(results)} results for: {test_query}")
            for i, result in enumerate(results, 1):
                title = result.metadata.get('title', 'Unknown')
                score = result.score
                print(f"   {i}. [{title}] (Score: {score:.3f})")
                
        except Exception as e:
            print(f"âš ï¸ Search test failed: {e}")
        
    except Exception as e:
        print(f"âŒ Error during index rebuild: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print(f"\nğŸš€ FAISS Index Successfully Rebuilt!")
    print(f"ğŸ“„ Your ARQA system now has access to all {len(documents):,} documents")
    print(f"ğŸŒŸ Including {len([d for d in documents if d.get('meta', {}).get('source') == 'wikipedia']):,} Wikipedia articles!")

if __name__ == "__main__":
    main()
