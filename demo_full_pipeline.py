#!/usr/bin/env python3
# filepath: c:\Users\a-ahm\Desktop\arqa\demo_full_pipeline.py
"""
Full Pipeline Demo: HTML Ingestion + Document Retrieval
Demonstrates the complete workflow from HTML processing to semantic search.
"""

import os
import sys
import json
from pathlib import Path

# Add the src directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def demo_full_pipeline():
    """Demonstrate the complete ARQA pipeline."""
    
    print("ğŸš€ ARQA Full Pipeline Demo")
    print("=" * 50)
    print("Phase 1: HTML Ingestion â†’ Phase 2: Document Retrieval")
    print("=" * 50)
    
    # ==========================================
    # PHASE 1: HTML INGESTION
    # ==========================================
    print("\nğŸ“‹ PHASE 1: HTML INGESTION")
    print("-" * 30)
    
    try:
        from arqa.simple_ingest import SimpleDocumentIngestor
        print("âœ… Imported SimpleDocumentIngestor")
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return False
    
    # Sample Arabic HTML content
    sample_html_docs = [
        {
            "url": "https://example.com/ai-article",
            "html": """
            <html>
            <head><title>Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</title></head>
            <body>
                <h1>Ù…Ù‚Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</h1>
                <p>Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ØªÙ‚Ù†ÙŠØ© Ø­Ø¯ÙŠØ«Ø© ØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø´Ø±ÙŠ ÙÙŠ Ø§Ù„Ø¢Ù„Ø§Øª ÙˆØ§Ù„Ø­Ø§Ø³ÙˆØ¨. 
                ÙŠØ´Ù…Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¬Ø§Ù„Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø«Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.</p>
                
                <h2>ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</h2>
                <p>ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ù…Ø«Ù„ Ø§Ù„Ø·Ø¨ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ù†Ù‚Ù„. 
                ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠØŒ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ´Ø®ÙŠØµ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„Ø¹Ù„Ø§Ø¬Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.</p>
                
                <h2>Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</h2>
                <p>ÙŠØªÙˆÙ‚Ø¹ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø£Ù† ÙŠÙ„Ø¹Ø¨ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¯ÙˆØ±Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹ ÙÙŠ ØªØ·ÙˆÙŠØ± Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© 
                ÙˆØ­Ù„ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ØªÙˆØ§Ø¬Ù‡ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©.</p>
            </body>
            </html>
            """
        },
        {
            "url": "https://example.com/education-article",
            "html": """
            <html>
            <head><title>Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ</title></head>
            <body>
                <h1>Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø­Ø¯ÙŠØ«</h1>
                <p>Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø£ØµØ¨Ø­ Ø¬Ø²Ø¡Ø§Ù‹ Ù„Ø§ ÙŠØªØ¬Ø²Ø£ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«. 
                ÙŠÙˆÙØ± Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù…Ø±ÙˆÙ†Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ù…ÙƒØ§Ù† Ù„Ù„Ø·Ù„Ø§Ø¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ†.</p>
                
                <h2>Ù…Ø²Ø§ÙŠØ§ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ</h2>
                <p>Ù…Ù† Ø£Ù‡Ù… Ù…Ø²Ø§ÙŠØ§ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª ÙˆÙ…Ù† Ø£ÙŠ Ù…ÙƒØ§Ù†. 
                ÙƒÙ…Ø§ ÙŠÙˆÙØ± Ø£Ø¯ÙˆØ§Øª ØªÙØ§Ø¹Ù„ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© ØªØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù….</p>
                
                <h2>Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ§Ù„Ø­Ù„ÙˆÙ„</h2>
                <p>Ø±ØºÙ… Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø¹Ø¯ÙŠØ¯Ø©ØŒ ÙŠÙˆØ§Ø¬Ù‡ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ØªØ­Ø¯ÙŠØ§Øª Ù…Ø«Ù„ Ù†Ù‚Øµ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± 
                ÙˆØ§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ù…Ù‡Ø§Ø±Ø§Øª ØªÙ‚Ù†ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©.</p>
            </body>
            </html>
            """
        },
        {
            "url": "https://example.com/renewable-energy",
            "html": """
            <html>
            <head><title>Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø©</title></head>
            <body>
                <h1>Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø©: Ù…Ø³ØªÙ‚Ø¨Ù„ Ù…Ø³ØªØ¯Ø§Ù…</h1>
                <p>Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø© ØªØ´Ù…Ù„ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙ†Ø¶Ø¨ Ù…Ø«Ù„ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© ÙˆØ·Ø§Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…Ø§Ø¦ÙŠØ©. 
                ØªØ¹ØªØ¨Ø± Ù‡Ø°Ù‡ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ù†Ø¸ÙŠÙØ§Ù‹ ÙˆÙ…Ø³ØªØ¯Ø§Ù…Ø§Ù‹ Ù„Ù„ÙˆÙ‚ÙˆØ¯ Ø§Ù„Ø£Ø­ÙÙˆØ±ÙŠ.</p>
                
                <h2>Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ©</h2>
                <p>Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ù‡ÙŠ Ø£ÙƒØ«Ø± Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø© ØªÙˆÙØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¶. 
                ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù„ÙˆØ§Ø­ Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø¶ÙˆØ¡ Ø§Ù„Ø´Ù…Ø³ Ø¥Ù„Ù‰ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù†Ø¸ÙŠÙØ©.</p>
                
                <h2>Ø·Ø§Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø­</h2>
                <p>Ø·Ø§Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø­ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ±Ø¨ÙŠÙ†Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…Ù† Ø­Ø±ÙƒØ© Ø§Ù„Ù‡ÙˆØ§Ø¡. 
                ØªØ¹ØªØ¨Ø± Ù…Ù† Ø£Ø³Ø±Ø¹ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø·Ø§Ù‚Ø© Ù†Ù…ÙˆØ§Ù‹ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù….</p>
            </body>
            </html>
            """
        }
    ]
    
    # Process HTML documents
    print("ğŸ”§ Processing HTML documents...")
    
    ingestor = SimpleDocumentIngestor(
        output_dir="./demo_output",
        chunk_size=200,
        chunk_overlap=50
    )
    
    processed_docs = []
    for doc in sample_html_docs:
        print(f"   ğŸ“„ Processing: {doc['url']}")
        result = ingestor.process_html(doc['url'], doc['html'])
        processed_docs.append(result)
    
    # Save processed documents
    output_file = "./demo_output/processed_documents.json"
    ingestor.save_documents(processed_docs, output_file)
    print(f"âœ… Phase 1 Complete: {len(processed_docs)} documents processed")
    
    # ==========================================
    # PHASE 2: DOCUMENT RETRIEVAL
    # ==========================================
    print("\nğŸ” PHASE 2: DOCUMENT RETRIEVAL")
    print("-" * 30)
    
    try:
        from arqa.retriever import ArabicDocumentRetriever
        print("âœ… Imported ArabicDocumentRetriever")
    except ImportError as e:
        print(f"âŒ Retriever import error: {e}")
        print("ğŸ’¡ Install dependencies: pip install torch transformers faiss-cpu tqdm numpy")
        return False
    
    # Convert processed documents to retriever format
    print("ğŸ”§ Converting documents for retrieval...")
    
    retriever_docs = []
    for doc in processed_docs:
        for chunk in doc.get('chunks', []):
            retriever_docs.append({
                'content': chunk['content'],
                'meta': {
                    'source': doc.get('source', 'unknown'),
                    'title': doc.get('title', 'No title'),
                    'url': doc.get('url', ''),
                    'chunk_index': chunk.get('chunk_index', 0)
                },
                'chunk_id': chunk.get('chunk_index', 0)
            })
    
    print(f"ğŸ“š Converted {len(retriever_docs)} document chunks")
    
    # Initialize retriever with AraDPR
    print("ğŸš€ Initializing retriever with AraDPR...")
    
    try:
        retriever = ArabicDocumentRetriever(
            model_name="abdoelsayed/AraDPR",
            index_path="./demo_output/faiss_index",
            documents_path="./demo_output/documents_metadata.json",
            top_k=3
        )
        
        # Add documents and create embeddings
        retriever.add_documents(retriever_docs)
        print("âœ… Phase 2 Complete: Documents indexed and ready for search")
        
    except Exception as e:
        print(f"âŒ Error initializing retriever: {e}")
        return False
    
    # ==========================================
    # PHASE 3: SEMANTIC SEARCH DEMO
    # ==========================================
    print("\nğŸ¯ PHASE 3: SEMANTIC SEARCH DEMO")
    print("-" * 30)
    
    # Test queries
    test_queries = [
        "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ",
        "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ",
        "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø©",
        "ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…",
        "Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ©",
        "ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠ"
    ]
    
    print("ğŸ” Testing semantic search with Arabic queries...")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*20} Query {i} {'='*20}")
        print(f"ğŸ” Ø§Ù„Ø³Ø¤Ø§Ù„: {query}")
        
        try:
            results = retriever.retrieve(query, top_k=2)
            
            if results:
                print("ğŸ“– Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
                for j, result in enumerate(results, 1):
                    print(f"\n   ğŸ“„ Ù†ØªÙŠØ¬Ø© {j} (Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {result.score:.3f})")
                    print(f"   ğŸ“° Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {result.meta.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}")
                    print(f"   ğŸ“ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {result.content[:120]}...")
                    print(f"   ğŸ”— Ø§Ù„Ù…ØµØ¯Ø±: {result.meta.get('url', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
            else:
                print("   âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
                
        except Exception as e:
            print(f"   âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
    
    # ==========================================
    # PHASE 4: MODEL SWITCHING DEMO
    # ==========================================
    print(f"\n{'='*50}")
    print("ğŸ”„ BONUS: MODEL SWITCHING DEMO")
    print("-" * 30)
    
    try:
        print("ğŸ”„ Switching to e5-arabic-base model...")
        retriever.switch_model("intfloat/e5-arabic-base")
        
        # Test with new model
        test_query = "Ø§Ù„ØªØ·ÙˆØ±Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©"
        print(f"ğŸ” Testing with new model: {test_query}")
        
        results = retriever.retrieve(test_query, top_k=2)
        if results:
            for j, result in enumerate(results, 1):
                print(f"   ğŸ“„ Result {j}: {result.content[:80]}...")
        
        print("âœ… Model switching successful!")
        
    except Exception as e:
        print(f"âš ï¸ Model switching failed: {e}")
        print("ğŸ’¡ This is normal - switching may fail due to model availability or memory")
    
    # ==========================================
    # SUMMARY
    # ==========================================
    print(f"\n{'='*50}")
    print("ğŸ“Š PIPELINE SUMMARY")
    print("-" * 30)
    
    stats = retriever.get_stats()
    print("âœ… Successfully completed:")
    print(f"   ğŸ“‹ Phase 1: Processed {len(processed_docs)} HTML documents")
    print(f"   ğŸ“š Phase 2: Indexed {stats['total_documents']} document chunks")  
    print(f"   ğŸ” Phase 3: Tested semantic search with {len(test_queries)} queries")
    print(f"   ğŸ¤– Model: {stats['model_name']}")
    print(f"   ğŸ’¾ Index size: {stats['index_size']} embeddings")
    
    print("\nğŸ‰ ARQA Pipeline Demo Complete!")
    print("\nğŸ“‹ What's working:")
    print("   âœ… HTML processing with Arabic normalization")
    print("   âœ… Document chunking and metadata extraction")
    print("   âœ… AraDPR embeddings with FAISS indexing")
    print("   âœ… Semantic search in Arabic")
    print("   âœ… Model switching capabilities")
    print("   âœ… Progress bars and user feedback")
    
    print("\nğŸ”„ Next steps:")
    print("   1. Add question answering (reader.py)")
    print("   2. Create REST API interface (api.py)")
    print("   3. Add more sophisticated text preprocessing")
    print("   4. Implement evaluation metrics")
    
    return True


if __name__ == "__main__":
    # Create output directory
    os.makedirs("demo_output", exist_ok=True)
    
    # Run the demo
    success = demo_full_pipeline()
    
    if not success:
        print("\nğŸ’¡ Installation help:")
        print("   Basic (Phase 1): pip install beautifulsoup4 lxml")
        print("   Full (Phase 2): pip install torch transformers faiss-cpu tqdm numpy")
